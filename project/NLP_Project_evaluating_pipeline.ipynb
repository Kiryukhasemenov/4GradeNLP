{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Project evaluating pipeline",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axg_OdYp-TNW",
        "colab_type": "text"
      },
      "source": [
        "## Preparing UDPipe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDDYjM-4-SiX",
        "colab_type": "code",
        "outputId": "71c829da-d161-4628-aa48-f468d560257e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "!wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2998/russian-syntagrus-ud-2.4-190531.udpipe?sequence=74&amp;isAllowed=y\n",
        "!mv russian-syntagrus-ud-2.4-190531.udpipe?sequence=74 russian-syntagrus-ud-2.4-190531.udpipe"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: amp: command not found\n",
            "--2019-12-24 23:02:05--  https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2998/russian-syntagrus-ud-2.4-190531.udpipe?sequence=74\n",
            "Resolving lindat.mff.cuni.cz (lindat.mff.cuni.cz)... 195.113.20.140\n",
            "Connecting to lindat.mff.cuni.cz (lindat.mff.cuni.cz)|195.113.20.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45859472 (44M) [application/octet-stream]\n",
            "Saving to: ‘russian-syntagrus-ud-2.4-190531.udpipe?sequence=74’\n",
            "\n",
            "russian-syntagrus-u 100%[===================>]  43.73M  8.25MB/s    in 6.5s    \n",
            "\n",
            "2019-12-24 23:02:14 (6.72 MB/s) - ‘russian-syntagrus-ud-2.4-190531.udpipe?sequence=74’ saved [45859472/45859472]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46JQShGd_WfT",
        "colab_type": "code",
        "outputId": "db9b8ac9-7032-42cf-ff0e-4dae95a581ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "!pip install ufal.udpipe"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ufal.udpipe\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\r\u001b[K     |█                               | 10kB 16.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 307kB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: ufal.udpipe\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp36-cp36m-linux_x86_64.whl size=5626009 sha256=3ce9085e02e36e0a961d76782afc6826e889ed251be894a204e50304c9b1c0a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "Successfully built ufal.udpipe\n",
            "Installing collected packages: ufal.udpipe\n",
            "Successfully installed ufal.udpipe-1.2.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY9dDcmb_XgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ufal.udpipe\n",
        "\n",
        "class UDPipeModel:\n",
        "    def __init__(self, path):\n",
        "        \"\"\"Load given model.\"\"\"\n",
        "        self.model = ufal.udpipe.Model.load(path)\n",
        "        if not self.model:\n",
        "            raise Exception(\"Cannot load UDPipe model from file '%s'\" % path)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenize the text and return list of ufal.udpipe.Sentence-s.\"\"\"\n",
        "        tokenizer = self.model.newTokenizer(self.model.DEFAULT)\n",
        "        if not tokenizer:\n",
        "            raise Exception(\"The model does not have a tokenizer\")\n",
        "        return self._read(text, tokenizer)\n",
        "\n",
        "    def read(self, text, in_format):\n",
        "        \"\"\"Load text in the given format (conllu|horizontal|vertical) and return list of ufal.udpipe.Sentence-s.\"\"\"\n",
        "        input_format = ufal.udpipe.InputFormat.newInputFormat(in_format)\n",
        "        if not input_format:\n",
        "            raise Exception(\"Cannot create input format '%s'\" % in_format)\n",
        "        return self._read(text, input_format)\n",
        "\n",
        "    def _read(self, text, input_format):\n",
        "        input_format.setText(text)\n",
        "        error = ufal.udpipe.ProcessingError()\n",
        "        sentences = []\n",
        "\n",
        "        sentence = ufal.udpipe.Sentence()\n",
        "        while input_format.nextSentence(sentence, error):\n",
        "            sentences.append(sentence)\n",
        "            sentence = ufal.udpipe.Sentence()\n",
        "        if error.occurred():\n",
        "            raise Exception(error.message)\n",
        "\n",
        "        return sentences\n",
        "\n",
        "    def tag(self, sentence):\n",
        "        \"\"\"Tag the given ufal.udpipe.Sentence (inplace).\"\"\"\n",
        "        self.model.tag(sentence, self.model.DEFAULT)\n",
        "\n",
        "    def parse(self, sentence):\n",
        "        \"\"\"Parse the given ufal.udpipe.Sentence (inplace).\"\"\"\n",
        "        self.model.parse(sentence, self.model.DEFAULT)\n",
        "\n",
        "    def write(self, sentences, out_format):\n",
        "        \"\"\"Write given ufal.udpipe.Sentence-s in the required format (conllu|horizontal|vertical).\"\"\"\n",
        "\n",
        "        output_format = ufal.udpipe.OutputFormat.newOutputFormat(out_format)\n",
        "        output = ''\n",
        "        for sentence in sentences:\n",
        "            output += output_format.writeSentence(sentence)\n",
        "        output += output_format.finishDocument()\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-V860sSB6NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word:\n",
        "\n",
        "  def __init__(self, conllu_str):\n",
        "    if not conllu_str:\n",
        "      raise ValueError(\"A valid word parse in conllu format should be passed.\")\n",
        "    if conllu_str[0] not in \"0123456789\":\n",
        "      raise ValueError(\"A valid word parse in conllu format should be passed.\")\n",
        "    conllu_str = conllu_str.split(\"\\t\")\n",
        "    self.position = int(conllu_str[0])\n",
        "    self.token = conllu_str[1]\n",
        "    self.lower = self.token.lower\n",
        "    self.lemma = conllu_str[2]\n",
        "    self.POS = conllu_str[3]\n",
        "    self.pos = self.POS\n",
        "    self.parse = None\n",
        "    if \"=\" in conllu_str[5]:\n",
        "      self.parse = {var.split(\"=\")[0]: var.split(\"=\")[1] for var in conllu_str[5].split(\"\\t\")}\n",
        "    if conllu_str[6] == \"0\":\n",
        "      self.head = None\n",
        "    else:\n",
        "      self.head = int(conllu_str[6])\n",
        "    self.dep = conllu_str[7]\n",
        "    self.space_after = True\n",
        "    self.spaces_before = \"\"\n",
        "    if conllu_str[9] != \"_\":\n",
        "      space_dict = {var.split(\"=\")[0]: var.split(\"=\")[1] for var in conllu_str[9].split(\"\\t\")}\n",
        "      if \"SpaceAfter\" in space_dict:\n",
        "        if space_dict[\"SpaceAfter\"] == \"No\":\n",
        "          self.space_after = False\n",
        "      if \"SpacesBefore\" in space_dict:\n",
        "        self.spaces_before = space_dict[\"SpacesBefore\"].decode(\"string_escape\")\n",
        "\n",
        "  def __repr__(self):\n",
        "    return self.token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqymoQ3bf0yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "class Model:\n",
        "\n",
        "  def __init__(self, udpipemodel):\n",
        "    self.model = udpipemodel\n",
        "  \n",
        "  def make_conllu(self, text):\n",
        "    sentences = self.model.tokenize(text)\n",
        "    for s in sentences:\n",
        "        self.model.tag(s)\n",
        "        self.model.parse(s)\n",
        "    conllu = self.model.write(sentences, \"conllu\")\n",
        "    return conllu\n",
        "\n",
        "  def parse_words(self, text):\n",
        "    conllu = self.make_conllu(text)\n",
        "    suitstrs = [s for s in conllu.split(\"\\n\") if s]\n",
        "    suitstrs = [s for s in suitstrs if s[0] in \"0123456789\"]\n",
        "    parse_res = [Word(suitstr) for suitstr in suitstrs]\n",
        "    return parse_res\n",
        "\n",
        "  def parse_text(self, text):\n",
        "    conllu = self.make_conllu(text)\n",
        "    sent_dict = OrderedDict()\n",
        "    sents = [sent for sent in conllu.split(\"# sent_id = \") if sent[0] in \"0123456789\"]\n",
        "    for sent in sents:\n",
        "      sent_num = int(sent[:sent.find(\"\\n\")])\n",
        "      word_strs = [s for s in sent.split(\"\\n\") if s][1:]\n",
        "      words = [Word(s) for s in word_strs if s[0] in \"0123456789\"]\n",
        "      sent_dict[sent_num] = OrderedDict()\n",
        "      for word in words:\n",
        "        sent_dict[sent_num][word.position-1] = word\n",
        "    return sent_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouiB1vPLQmos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ud_model = Model(UDPipeModel('russian-syntagrus-ud-2.4-190531.udpipe'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCXu2fj_7L3k",
        "colab_type": "text"
      },
      "source": [
        "## Preparing spelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLvW2Nwo7MKD",
        "colab_type": "code",
        "outputId": "2741625f-bd0b-4709-c44a-5af27dc1aacd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt-get install swig\n",
        "!easy_install-3.6 jamspell\n",
        "!cp -r /usr/local/lib/python3.6/dist-packages/jamspell-0.0.11-py3.6-linux-x86_64.egg/EGG-INFO /usr/local/lib/python3.6/dist-packages/jamspell-0.0.11-py3.6-linux-x86_64.egg-info\n",
        "!cp -r /usr/local/lib/python3.6/dist-packages/jamspell-0.0.11-py3.6-linux-x86_64.egg /usr/local/lib/python3.6/dist-packages/jamspell\n",
        "!wget https://github.com/bakwc/JamSpell-models/raw/master/ru.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 3s (419 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 135004 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "WARNING: The easy_install command is deprecated and will be removed in a future version.\n",
            "Searching for jamspell\n",
            "Reading https://pypi.org/simple/jamspell/\n",
            "Downloading https://files.pythonhosted.org/packages/5a/16/0a808e926a835604007066085cb2183b337694a06240a99945b31fa14f27/jamspell-0.0.11.tar.gz#sha256=6dcaf1ae631af6c0904f9ba016bf2404e930237eb73e4d471ee92a77327af8f1\n",
            "Best match: jamspell 0.0.11\n",
            "Processing jamspell-0.0.11.tar.gz\n",
            "Writing /tmp/easy_install-dy_v9u9q/jamspell-0.0.11/setup.cfg\n",
            "Running jamspell-0.0.11/setup.py -q bdist_egg --dist-dir /tmp/easy_install-dy_v9u9q/jamspell-0.0.11/egg-dist-tmp-9a69o7d_\n",
            "creating /usr/local/lib/python3.6/dist-packages/jamspell-0.0.11-py3.6-linux-x86_64.egg\n",
            "Extracting jamspell-0.0.11-py3.6-linux-x86_64.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding jamspell 0.0.11 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/jamspell-0.0.11-py3.6-linux-x86_64.egg\n",
            "Processing dependencies for jamspell\n",
            "Finished processing dependencies for jamspell\n",
            "--2019-12-24 23:04:25--  https://github.com/bakwc/JamSpell-models/raw/master/ru.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/bakwc/JamSpell-models/master/ru.tar.gz [following]\n",
            "--2019-12-24 23:04:26--  https://raw.githubusercontent.com/bakwc/JamSpell-models/master/ru.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39663667 (38M) [application/octet-stream]\n",
            "Saving to: ‘ru.tar.gz’\n",
            "\n",
            "ru.tar.gz           100%[===================>]  37.83M  46.8MB/s    in 0.8s    \n",
            "\n",
            "2019-12-24 23:04:29 (46.8 MB/s) - ‘ru.tar.gz’ saved [39663667/39663667]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-oErmBA8pRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "\n",
        "tar = tarfile.open(\"ru.tar.gz\")\n",
        "tar.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj-M28zNNqji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jamspell import jamspell\n",
        "\n",
        "class Jamspell:\n",
        "  \n",
        "  def __init__(self, path):\n",
        "    self.instance = jamspell.TSpellCorrector()\n",
        "    self.instance.LoadLangModel(path)\n",
        "\t\n",
        "  def correct(self, text):\n",
        "    return self.instance.FixFragment(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoHKfJjeN5qD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spellchecker = Jamspell('ru_small.bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKYXr8ViQuH1",
        "colab_type": "text"
      },
      "source": [
        "## Preparing vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOUIKbApQt4C",
        "colab_type": "code",
        "outputId": "f3a22a6d-3283-4dbb-f609-c8368f63f854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!wget 'http://vectors.nlpl.eu/repository/11/185.zip'\n",
        "!unzip '185.zip'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-24 23:05:07--  http://vectors.nlpl.eu/repository/11/185.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 639268530 (610M) [application/zip]\n",
            "Saving to: ‘185.zip’\n",
            "\n",
            "185.zip             100%[===================>] 609.65M  9.76MB/s    in 71s     \n",
            "\n",
            "2019-12-24 23:06:19 (8.61 MB/s) - ‘185.zip’ saved [639268530/639268530]\n",
            "\n",
            "Archive:  185.zip\n",
            "  inflating: README                  \n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW1RH5wQQwWC",
        "colab_type": "code",
        "outputId": "1a278bbb-2300-488a-ee50-8d6eb8e00d05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "source": [
        "!wget https://www.dropbox.com/s/q1w4ftavn69383c/mean_vectors.pk?dl=0\n",
        "!mv mean_vectors.pk?dl=0 mean_vectors.pk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-24 23:06:37--  https://www.dropbox.com/s/q1w4ftavn69383c/mean_vectors.pk?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.1, 2620:100:6032:1::a27d:5201\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/q1w4ftavn69383c/mean_vectors.pk [following]\n",
            "--2019-12-24 23:06:37--  https://www.dropbox.com/s/raw/q1w4ftavn69383c/mean_vectors.pk\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc0cb645cc2bd8a883d6af9b8b99.dl.dropboxusercontent.com/cd/0/inline/Au3vs6uHNXM3AQVA9NIy18KeD2Xj6Bxqe0910MhnYk6JOEOrvFxehySjPnRsMkM9sy9FFzPHKw9P7vaZPn6XrRhKLHq7xaicAeRR_ALj6MMm2rjz3G7t8IrqYHrSLh1NOvU/file# [following]\n",
            "--2019-12-24 23:06:37--  https://uc0cb645cc2bd8a883d6af9b8b99.dl.dropboxusercontent.com/cd/0/inline/Au3vs6uHNXM3AQVA9NIy18KeD2Xj6Bxqe0910MhnYk6JOEOrvFxehySjPnRsMkM9sy9FFzPHKw9P7vaZPn6XrRhKLHq7xaicAeRR_ALj6MMm2rjz3G7t8IrqYHrSLh1NOvU/file\n",
            "Resolving uc0cb645cc2bd8a883d6af9b8b99.dl.dropboxusercontent.com (uc0cb645cc2bd8a883d6af9b8b99.dl.dropboxusercontent.com)... 162.125.82.6, 2620:100:6032:6::a27d:5206\n",
            "Connecting to uc0cb645cc2bd8a883d6af9b8b99.dl.dropboxusercontent.com (uc0cb645cc2bd8a883d6af9b8b99.dl.dropboxusercontent.com)|162.125.82.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/Au2vDjN4PucUY2bZc7c_yS-e5z0xw5xmps47jb_IQ6qUut3T71Ykhzk4dndgCOMZLwDUaNo7lmnbqUVwXuht0vcIJda0pginBLaOtNCHfd8W86FdlPc3Eli4SA21pydNC4v2DfKUgy3PgMe8V5PfVhEHyQiRuw74llt3K3UPD7uoa7Xbi7OZW9XVwfwrHDn8pB3CzzR5IiMXdceYPjgKwFP-JE9jvhzwpSh2fb9kDxLJH1JBe_KEXF4Y7WO6-z_FfN7mLf7oQZgA0UGj75Hl-w9rgSXW4PAi8GVRaSSNYVFp5McuIzbc3YlwG4g4w-S-Y_XrRUSI-hoUygrH3eqrrxlKIqv1ZLZkjvPEX4yJlsRNbg/file [following]\n",
            "--2019-12-24 23:06:38--  https://uc0cb645cc2bd8a883d6af9b8b99.dl.dropboxusercontent.com/cd/0/inline2/Au2vDjN4PucUY2bZc7c_yS-e5z0xw5xmps47jb_IQ6qUut3T71Ykhzk4dndgCOMZLwDUaNo7lmnbqUVwXuht0vcIJda0pginBLaOtNCHfd8W86FdlPc3Eli4SA21pydNC4v2DfKUgy3PgMe8V5PfVhEHyQiRuw74llt3K3UPD7uoa7Xbi7OZW9XVwfwrHDn8pB3CzzR5IiMXdceYPjgKwFP-JE9jvhzwpSh2fb9kDxLJH1JBe_KEXF4Y7WO6-z_FfN7mLf7oQZgA0UGj75Hl-w9rgSXW4PAi8GVRaSSNYVFp5McuIzbc3YlwG4g4w-S-Y_XrRUSI-hoUygrH3eqrrxlKIqv1ZLZkjvPEX4yJlsRNbg/file\n",
            "Reusing existing connection to uc0cb645cc2bd8a883d6af9b8b99.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5150 (5.0K) [application/x-tex-pk]\n",
            "Saving to: ‘mean_vectors.pk?dl=0’\n",
            "\n",
            "mean_vectors.pk?dl= 100%[===================>]   5.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-12-24 23:06:38 (699 MB/s) - ‘mean_vectors.pk?dl=0’ saved [5150/5150]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXtCHMWyQrSC",
        "colab_type": "code",
        "outputId": "0b84c1ba-5601-405a-ab81-e9b5cf1f7c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "w2v_model = KeyedVectors.load_word2vec_format('model.bin', binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MX77gHWY-ts",
        "colab_type": "text"
      },
      "source": [
        "## Downloading wordlists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9pEjTjzf43v",
        "colab_type": "code",
        "outputId": "81388108-a365-426a-b7ff-309f161e0b2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        }
      },
      "source": [
        "!wget https://www.dropbox.com/s/x6yiaveef5snf94/lexicon_by_parts.zip?dl=0\n",
        "!mv lexicon_by_parts.zip?dl=0 lexicon_by_parts.zip\n",
        "!unzip lexicon_by_parts.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-24 23:06:55--  https://www.dropbox.com/s/x6yiaveef5snf94/lexicon_by_parts.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.1, 2620:100:6032:1::a27d:5201\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/x6yiaveef5snf94/lexicon_by_parts.zip [following]\n",
            "--2019-12-24 23:06:56--  https://www.dropbox.com/s/raw/x6yiaveef5snf94/lexicon_by_parts.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc60886dd47cfc5fb06002622b40.dl.dropboxusercontent.com/cd/0/inline/Au2ovrhM5fzmN4kxz_cGJfLeFHkxYHlX3C4_wfnCcbqyVwV0gxGr-K_bkbsnFg1-yPXrn_I1UJpZn1gnNfqvTcbUzJ8ZMxPxvP0qCX0Y6u7aqSNCjPAQhBRQgno9yilG1w4/file# [following]\n",
            "--2019-12-24 23:06:56--  https://uc60886dd47cfc5fb06002622b40.dl.dropboxusercontent.com/cd/0/inline/Au2ovrhM5fzmN4kxz_cGJfLeFHkxYHlX3C4_wfnCcbqyVwV0gxGr-K_bkbsnFg1-yPXrn_I1UJpZn1gnNfqvTcbUzJ8ZMxPxvP0qCX0Y6u7aqSNCjPAQhBRQgno9yilG1w4/file\n",
            "Resolving uc60886dd47cfc5fb06002622b40.dl.dropboxusercontent.com (uc60886dd47cfc5fb06002622b40.dl.dropboxusercontent.com)... 162.125.82.6, 2620:100:6032:6::a27d:5206\n",
            "Connecting to uc60886dd47cfc5fb06002622b40.dl.dropboxusercontent.com (uc60886dd47cfc5fb06002622b40.dl.dropboxusercontent.com)|162.125.82.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/Au2FK5hZ81brzWF5NgLWHEWgpm88mSnZRPmHitNkpLLvJ5yZpvSupqZbPCfTuq4BfzMJ52m2MdJR0cxGOYh8gP-A0y6YgcmORfGdIEgKOeSAMZh8TI-E6n9ZsFtterRHQYqPKh3YPUhy4Z3ecwftKDULASNH1HoFmQ_Gy96OHO3InNrqMGXEnaR1KsQVEhKz6gbACyqjw28Mn-134OwQC5TTBfTT0F3lO9xNh1IJhX-4SlG8OjRRDOnIrCxaeqLiuTDhyBmAFn2tHX9MYCZPYWkwSjsupypxv82nP2Ngj6o7R99pktQxWXryGHnCZB32H6cRXMXqYTvOixnTpNpz7K2xPDvGM2h1KdRnxP0a4l9PhA/file [following]\n",
            "--2019-12-24 23:06:56--  https://uc60886dd47cfc5fb06002622b40.dl.dropboxusercontent.com/cd/0/inline2/Au2FK5hZ81brzWF5NgLWHEWgpm88mSnZRPmHitNkpLLvJ5yZpvSupqZbPCfTuq4BfzMJ52m2MdJR0cxGOYh8gP-A0y6YgcmORfGdIEgKOeSAMZh8TI-E6n9ZsFtterRHQYqPKh3YPUhy4Z3ecwftKDULASNH1HoFmQ_Gy96OHO3InNrqMGXEnaR1KsQVEhKz6gbACyqjw28Mn-134OwQC5TTBfTT0F3lO9xNh1IJhX-4SlG8OjRRDOnIrCxaeqLiuTDhyBmAFn2tHX9MYCZPYWkwSjsupypxv82nP2Ngj6o7R99pktQxWXryGHnCZB32H6cRXMXqYTvOixnTpNpz7K2xPDvGM2h1KdRnxP0a4l9PhA/file\n",
            "Reusing existing connection to uc60886dd47cfc5fb06002622b40.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19060 (19K) [application/zip]\n",
            "Saving to: ‘lexicon_by_parts.zip?dl=0’\n",
            "\n",
            "lexicon_by_parts.zi 100%[===================>]  18.61K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-12-24 23:06:57 (246 MB/s) - ‘lexicon_by_parts.zip?dl=0’ saved [19060/19060]\n",
            "\n",
            "Archive:  lexicon_by_parts.zip\n",
            "replace lexicon_by_parts/bigrams_food_negative.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: lexicon_by_parts/bigrams_food_negative.txt  \n",
            "replace lexicon_by_parts/bigrams_food_positive.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: lexicon_by_parts/bigrams_food_positive.txt  \n",
            "replace lexicon_by_parts/bigrams_service_negative.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: lexicon_by_parts/bigrams_service_negative.txt  \n",
            "replace lexicon_by_parts/bigrams_service_positive.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: lexicon_by_parts/bigrams_service_positive.txt  \n",
            "replace lexicon_by_parts/unigrams_food_negative.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: lexicon_by_parts/unigrams_food_negative.txt  \n",
            "replace lexicon_by_parts/unigrams_food_positive.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: lexicon_by_parts/unigrams_food_positive.txt  \n",
            "replace lexicon_by_parts/unigrams_service_negative.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: lexicon_by_parts/unigrams_service_negative.txt  \n",
            "replace lexicon_by_parts/unigrams_service_positive.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: lexicon_by_parts/unigrams_service_positive.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58rv9saGZCu_",
        "colab_type": "text"
      },
      "source": [
        "## Building evaluator pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ICANHQiRbDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbfUCKb8e4A6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Evaluator:\n",
        "\n",
        "  def load_wordlist_uni(self, path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as inf:\n",
        "      wordset = set([w.split(\"\\t\")[1] for w in inf.read().split(\"\\n\") if w])\n",
        "    return wordset\n",
        "  \n",
        "  def load_wordlist_bi(self, path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as inf:\n",
        "      wordset = set([eval(w.split(\"\\t\")[1]) for w in inf.read().split(\"\\n\") if w])\n",
        "    return wordset\n",
        "\n",
        "  def __init__(self, ud_model, w2v_model, spellchecker, lists_path, vectors_path):\n",
        "    self._upch = 200\n",
        "    self.food_0_uni = self.load_wordlist_uni(os.path.join(lists_path, \"unigrams_food_negative.txt\"))\n",
        "    self.food_1_uni = self.load_wordlist_uni(os.path.join(lists_path, \"unigrams_food_positive.txt\"))\n",
        "    self.food_0_bi = self.load_wordlist_bi(os.path.join(lists_path, \"bigrams_food_negative.txt\"))\n",
        "    self.food_1_bi = self.load_wordlist_bi(os.path.join(lists_path, \"bigrams_food_positive.txt\"))\n",
        "    self.service_0_uni = self.load_wordlist_uni(os.path.join(lists_path, \"unigrams_service_negative.txt\"))\n",
        "    self.service_1_uni = self.load_wordlist_uni(os.path.join(lists_path, \"unigrams_service_positive.txt\"))\n",
        "    self.service_0_bi = self.load_wordlist_bi(os.path.join(lists_path, \"bigrams_service_negative.txt\"))\n",
        "    self.service_1_bi = self.load_wordlist_bi(os.path.join(lists_path, \"bigrams_service_positive.txt\"))\n",
        "    self.model = ud_model\n",
        "    self.spellchecker = spellchecker\n",
        "    self.w2v_model = w2v_model\n",
        "    with open(vectors_path, 'rb') as f:\n",
        "        mean_vectors_dict = pickle.load(f)\n",
        "\n",
        "    self.pos_food = mean_vectors_dict['pos_food']\n",
        "    self.neg_food = mean_vectors_dict['neg_food']\n",
        "    self.pos_service = mean_vectors_dict['pos_service']\n",
        "    self.neg_service = mean_vectors_dict['neg_service']\n",
        "\n",
        "  def printstr(self, coded_str):\n",
        "    sp = coded_str.split(\"\\t\")\n",
        "    sent_id = ord(sp[0]) - self._upch\n",
        "    word_id = ord(sp[1]) - self._upch\n",
        "    out = \"\\t\".join([str(sent_id), str(word_id), \"\\t\".join(sp[2:])])\n",
        "    return out\n",
        "\n",
        "  def vector_comparison(self, mean_vec_spec, mean_vec_comp, word_lemma):\n",
        "    try:\n",
        "      grammeme_vec = self.w2v_model[word_lemma]\n",
        "    except:\n",
        "      grammeme_vec = np.zeros((1,300))\n",
        "    grammeme_vec = grammeme_vec.reshape(1, -1)\n",
        "    \n",
        "    cos_sim_spec = cosine_similarity(mean_vec_spec.reshape(1, -1), grammeme_vec)[0][0]\n",
        "    cos_sim_comp = cosine_similarity(mean_vec_comp.reshape(1, -1), grammeme_vec)[0][0]\n",
        "    \n",
        "    if cos_sim_spec > 0.5 and (cos_sim_spec - cos_sim_comp) > 0.2:\n",
        "      result = True\n",
        "    else:\n",
        "      result = False\n",
        "    return result\n",
        "\n",
        "  def process_uni(self, word):\n",
        "    out = []\n",
        "    grammeme = word.lemma + \"_\" + word.pos\n",
        "    if grammeme in self.food_0_uni or self.vector_comparison(self.neg_food, self.pos_food, grammeme):\n",
        "      out.append(chr(self._upch + word.position) + \"\\tFood\\t0\")\n",
        "    if grammeme in self.food_1_uni or self.vector_comparison(self.pos_food, self.neg_food, grammeme):\n",
        "      out.append(chr(self._upch + word.position) + \"\\tFood\\t1\")\n",
        "    if grammeme in self.service_0_uni or self.vector_comparison(self.neg_service, self.pos_service, grammeme):\n",
        "      out.append(chr(self._upch + word.position) + \"\\tService\\t0\")\n",
        "    if grammeme in self.service_1_uni or self.vector_comparison(self.pos_service, self.neg_service, grammeme):\n",
        "      out.append(chr(self._upch + word.position) + \"\\tService\\t1\")\n",
        "    \n",
        "    return out\n",
        "\n",
        "  def process_bi(self, bigram):\n",
        "    out = []\n",
        "    gramm_pair = (bigram[0].lemma + \"_\" + bigram[0].pos, bigram[1].lemma + \"_\" + bigram[1].pos)\n",
        "    if gramm_pair in self.food_0_bi:\n",
        "      out.append(chr(self._upch + bigram[0].position) + \"\\tFood\\t0\")\n",
        "      out.append(chr(self._upch + bigram[1].position) + \"\\tFood\\t0\")\n",
        "    if gramm_pair in self.food_1_bi:\n",
        "      out.append(chr(self._upch + bigram[0].position) + \"\\tFood\\t1\")\n",
        "      out.append(chr(self._upch + bigram[1].position) + \"\\tFood\\t1\")\n",
        "    if gramm_pair in self.service_0_bi:\n",
        "      out.append(chr(self._upch + bigram[0].position) + \"\\tService\\t0\")\n",
        "      out.append(chr(self._upch + bigram[1].position) + \"\\tService\\t0\")\n",
        "    if gramm_pair in self.service_1_bi:\n",
        "      out.append(chr(self._upch + bigram[0].position) + \"\\tService\\t1\")\n",
        "      out.append(chr(self._upch + bigram[1].position) + \"\\tService\\t1\")\n",
        "    return out\n",
        "\n",
        "  def process_text(self, text):\n",
        "    text = self.spellchecker.correct(text)\n",
        "    text_parse = self.model.parse_text(text)\n",
        "    out = []\n",
        "    for sent_id in text_parse:\n",
        "      SID = chr(self._upch + sent_id)\n",
        "      sent = text_parse[sent_id]\n",
        "      out += [SID + \"\\t\" + res for res in self.process_uni(sent[0])]\n",
        "      bigrams = [(sent[i], sent[i+1]) for i in range(len(sent) - 1)]\n",
        "      for bigram in bigrams:\n",
        "          out += [SID + \"\\t\" + res for res in self.process_bi(bigram)]\n",
        "          out += [SID + \"\\t\" + res for res in self.process_uni(bigram[1])]\n",
        "    out = sorted(list(set(out)))\n",
        "    outstr = \"\\n\".join(self.printstr(s) for s in out)\n",
        "    return outstr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq8y4vFJQ8C8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluator = Evaluator(ud_model, w2v_model, spellchecker, './lexicon_by_parts', 'mean_vectors.pk')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL-C5-_kZIv5",
        "colab_type": "text"
      },
      "source": [
        "## Predicting!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbAYiVe-Rzkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_text = \"\"\"— Тут был хороший бо-орщь, с капусткой, но не красный. Так... Сасисачки. Ну, ещё есть какой-то непонятный салаД, куда крошаД морковку, капусту и яблоки с ананасами (смеётся). Вообще он меня бесит. Вот... Эщо чоо... Вкусный чай, он так уталяет жажду (машет руками), я чувствую себя человеком! Вот. Всё.\n",
        "— (голос за кадром) Как фамилия ваша?\n",
        "— Я Никита Литвинков. (реверанс)\n",
        "— (голос за кадром) Спасибо...\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVrAjR4VRlyf",
        "colab_type": "code",
        "outputId": "18fcef93-aefa-4bbc-94f2-056a26312e22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(evaluator.process_text(sample_text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\t3\tService\t1\n",
            "1\t4\tService\t1\n",
            "8\t1\tFood\t1\n",
            "11\t15\tService\t1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}