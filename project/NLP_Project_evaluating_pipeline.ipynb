{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Project evaluating pipeline",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axg_OdYp-TNW",
        "colab_type": "text"
      },
      "source": [
        "## Preparing UDPipe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDDYjM-4-SiX",
        "colab_type": "code",
        "outputId": "3ab048a2-6587-4115-c79a-15aa1b94f5f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "!wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2998/russian-syntagrus-ud-2.4-190531.udpipe?sequence=74&amp;isAllowed=y\n",
        "!mv russian-syntagrus-ud-2.4-190531.udpipe?sequence=74 russian-syntagrus-ud-2.4-190531.udpipe"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: amp: command not found\n",
            "--2019-12-25 18:43:34--  https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2998/russian-syntagrus-ud-2.4-190531.udpipe?sequence=74\n",
            "Resolving lindat.mff.cuni.cz (lindat.mff.cuni.cz)... 195.113.20.140\n",
            "Connecting to lindat.mff.cuni.cz (lindat.mff.cuni.cz)|195.113.20.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45859472 (44M) [application/octet-stream]\n",
            "Saving to: ‘russian-syntagrus-ud-2.4-190531.udpipe?sequence=74’\n",
            "\n",
            "russian-syntagrus-u 100%[===================>]  43.73M  10.9MB/s    in 4.6s    \n",
            "\n",
            "2019-12-25 18:43:41 (9.56 MB/s) - ‘russian-syntagrus-ud-2.4-190531.udpipe?sequence=74’ saved [45859472/45859472]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46JQShGd_WfT",
        "colab_type": "code",
        "outputId": "c170652c-52bc-457e-edbd-2faad39f33da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "!pip install ufal.udpipe"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ufal.udpipe\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\r\u001b[K     |█                               | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 30kB 4.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 61kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 71kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 92kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 112kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 122kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 133kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 143kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 153kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 163kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 174kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 184kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 194kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 204kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 215kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 225kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 235kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 245kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 256kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 266kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 276kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 286kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 296kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 307kB 4.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: ufal.udpipe\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp36-cp36m-linux_x86_64.whl size=5625720 sha256=a80e624292b5d39b839cbe4cfef120543284429bf980d0ed8916168208e591f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "Successfully built ufal.udpipe\n",
            "Installing collected packages: ufal.udpipe\n",
            "Successfully installed ufal.udpipe-1.2.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY9dDcmb_XgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ufal.udpipe\n",
        "\n",
        "class UDPipeModel:\n",
        "    def __init__(self, path):\n",
        "        \"\"\"Load given model.\"\"\"\n",
        "        self.model = ufal.udpipe.Model.load(path)\n",
        "        if not self.model:\n",
        "            raise Exception(\"Cannot load UDPipe model from file '%s'\" % path)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenize the text and return list of ufal.udpipe.Sentence-s.\"\"\"\n",
        "        tokenizer = self.model.newTokenizer(self.model.DEFAULT)\n",
        "        if not tokenizer:\n",
        "            raise Exception(\"The model does not have a tokenizer\")\n",
        "        return self._read(text, tokenizer)\n",
        "\n",
        "    def read(self, text, in_format):\n",
        "        \"\"\"Load text in the given format (conllu|horizontal|vertical) and return list of ufal.udpipe.Sentence-s.\"\"\"\n",
        "        input_format = ufal.udpipe.InputFormat.newInputFormat(in_format)\n",
        "        if not input_format:\n",
        "            raise Exception(\"Cannot create input format '%s'\" % in_format)\n",
        "        return self._read(text, input_format)\n",
        "\n",
        "    def _read(self, text, input_format):\n",
        "        input_format.setText(text)\n",
        "        error = ufal.udpipe.ProcessingError()\n",
        "        sentences = []\n",
        "\n",
        "        sentence = ufal.udpipe.Sentence()\n",
        "        while input_format.nextSentence(sentence, error):\n",
        "            sentences.append(sentence)\n",
        "            sentence = ufal.udpipe.Sentence()\n",
        "        if error.occurred():\n",
        "            raise Exception(error.message)\n",
        "\n",
        "        return sentences\n",
        "\n",
        "    def tag(self, sentence):\n",
        "        \"\"\"Tag the given ufal.udpipe.Sentence (inplace).\"\"\"\n",
        "        self.model.tag(sentence, self.model.DEFAULT)\n",
        "\n",
        "    def parse(self, sentence):\n",
        "        \"\"\"Parse the given ufal.udpipe.Sentence (inplace).\"\"\"\n",
        "        self.model.parse(sentence, self.model.DEFAULT)\n",
        "\n",
        "    def write(self, sentences, out_format):\n",
        "        \"\"\"Write given ufal.udpipe.Sentence-s in the required format (conllu|horizontal|vertical).\"\"\"\n",
        "\n",
        "        output_format = ufal.udpipe.OutputFormat.newOutputFormat(out_format)\n",
        "        output = ''\n",
        "        for sentence in sentences:\n",
        "            output += output_format.writeSentence(sentence)\n",
        "        output += output_format.finishDocument()\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-V860sSB6NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word:\n",
        "\n",
        "  def __init__(self, conllu_str):\n",
        "    if not conllu_str:\n",
        "      raise ValueError(\"A valid word parse in conllu format should be passed.\")\n",
        "    if conllu_str[0] not in \"0123456789\":\n",
        "      raise ValueError(\"A valid word parse in conllu format should be passed.\")\n",
        "    conllu_str = conllu_str.split(\"\\t\")\n",
        "    self.position = int(conllu_str[0])\n",
        "    self.token = conllu_str[1]\n",
        "    self.lower = self.token.lower\n",
        "    self.lemma = conllu_str[2]\n",
        "    self.POS = conllu_str[3]\n",
        "    self.pos = self.POS\n",
        "    self.parse = None\n",
        "    if \"=\" in conllu_str[5]:\n",
        "      self.parse = {var.split(\"=\")[0]: var.split(\"=\")[1] for var in conllu_str[5].split(\"\\t\")}\n",
        "    if conllu_str[6] == \"0\":\n",
        "      self.head = None\n",
        "    else:\n",
        "      self.head = int(conllu_str[6])\n",
        "    self.dep = conllu_str[7]\n",
        "    self.space_after = True\n",
        "    self.spaces_before = \"\"\n",
        "    if conllu_str[9] != \"_\":\n",
        "      space_dict = {var.split(\"=\")[0]: var.split(\"=\")[1] for var in conllu_str[9].split(\"\\t\")}\n",
        "      if \"SpaceAfter\" in space_dict:\n",
        "        if space_dict[\"SpaceAfter\"] == \"No\":\n",
        "          self.space_after = False\n",
        "      if \"SpacesBefore\" in space_dict:\n",
        "        self.spaces_before = space_dict[\"SpacesBefore\"].decode(\"string_escape\")\n",
        "\n",
        "  def __repr__(self):\n",
        "    return self.token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqymoQ3bf0yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "class Model:\n",
        "\n",
        "  def __init__(self, udpipemodel):\n",
        "    self.model = udpipemodel\n",
        "  \n",
        "  def make_conllu(self, text):\n",
        "    sentences = self.model.tokenize(text)\n",
        "    for s in sentences:\n",
        "        self.model.tag(s)\n",
        "        self.model.parse(s)\n",
        "    conllu = self.model.write(sentences, \"conllu\")\n",
        "    return conllu\n",
        "\n",
        "  def parse_words(self, text):\n",
        "    conllu = self.make_conllu(text)\n",
        "    suitstrs = [s for s in conllu.split(\"\\n\") if s]\n",
        "    suitstrs = [s for s in suitstrs if s[0] in \"0123456789\"]\n",
        "    parse_res = [Word(suitstr) for suitstr in suitstrs]\n",
        "    return parse_res\n",
        "\n",
        "  def parse_text(self, text):\n",
        "    conllu = self.make_conllu(text)\n",
        "    sent_dict = OrderedDict()\n",
        "    sents = [sent for sent in conllu.split(\"# sent_id = \") if sent[0] in \"0123456789\"]\n",
        "    for sent in sents:\n",
        "      sent_num = int(sent[:sent.find(\"\\n\")])\n",
        "      word_strs = [s for s in sent.split(\"\\n\") if s][1:]\n",
        "      words = [Word(s) for s in word_strs if s[0] in \"0123456789\"]\n",
        "      sent_dict[sent_num] = OrderedDict()\n",
        "      for word in words:\n",
        "        sent_dict[sent_num][word.position-1] = word\n",
        "    return sent_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouiB1vPLQmos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ud_model = Model(UDPipeModel('russian-syntagrus-ud-2.4-190531.udpipe'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCXu2fj_7L3k",
        "colab_type": "text"
      },
      "source": [
        "## Preparing spelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLvW2Nwo7MKD",
        "colab_type": "code",
        "outputId": "9d6c7a5f-5a49-4e85-f155-ef3f877633a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt-get install swig\n",
        "!easy_install-3.6 jamspell\n",
        "!cp -r /usr/local/lib/python3.6/dist-packages/jamspell-0.0.11-py3.6-linux-x86_64.egg/EGG-INFO /usr/local/lib/python3.6/dist-packages/jamspell-0.0.11-py3.6-linux-x86_64.egg-info\n",
        "!cp -r /usr/local/lib/python3.6/dist-packages/jamspell-0.0.11-py3.6-linux-x86_64.egg /usr/local/lib/python3.6/dist-packages/jamspell\n",
        "!wget https://github.com/bakwc/JamSpell-models/raw/master/ru.tar.gz"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 1s (844 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 135004 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "WARNING: The easy_install command is deprecated and will be removed in a future version.\n",
            "Searching for jamspell\n",
            "Reading https://pypi.org/simple/jamspell/\n",
            "Downloading https://files.pythonhosted.org/packages/5a/16/0a808e926a835604007066085cb2183b337694a06240a99945b31fa14f27/jamspell-0.0.11.tar.gz#sha256=6dcaf1ae631af6c0904f9ba016bf2404e930237eb73e4d471ee92a77327af8f1\n",
            "Best match: jamspell 0.0.11\n",
            "Processing jamspell-0.0.11.tar.gz\n",
            "Writing /tmp/easy_install-u8fjurw2/jamspell-0.0.11/setup.cfg\n",
            "Running jamspell-0.0.11/setup.py -q bdist_egg --dist-dir /tmp/easy_install-u8fjurw2/jamspell-0.0.11/egg-dist-tmp-toln2ap5\n",
            "creating /usr/local/lib/python3.6/dist-packages/jamspell-0.0.11-py3.6-linux-x86_64.egg\n",
            "Extracting jamspell-0.0.11-py3.6-linux-x86_64.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding jamspell 0.0.11 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/jamspell-0.0.11-py3.6-linux-x86_64.egg\n",
            "Processing dependencies for jamspell\n",
            "Finished processing dependencies for jamspell\n",
            "--2019-12-25 18:45:53--  https://github.com/bakwc/JamSpell-models/raw/master/ru.tar.gz\n",
            "Resolving github.com (github.com)... 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/bakwc/JamSpell-models/master/ru.tar.gz [following]\n",
            "--2019-12-25 18:45:53--  https://raw.githubusercontent.com/bakwc/JamSpell-models/master/ru.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39663667 (38M) [application/octet-stream]\n",
            "Saving to: ‘ru.tar.gz’\n",
            "\n",
            "ru.tar.gz           100%[===================>]  37.83M  75.0MB/s    in 0.5s    \n",
            "\n",
            "2019-12-25 18:45:54 (75.0 MB/s) - ‘ru.tar.gz’ saved [39663667/39663667]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-oErmBA8pRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "\n",
        "tar = tarfile.open(\"ru.tar.gz\")\n",
        "tar.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj-M28zNNqji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jamspell import jamspell\n",
        "\n",
        "class Jamspell:\n",
        "  \n",
        "  def __init__(self, path):\n",
        "    self.instance = jamspell.TSpellCorrector()\n",
        "    self.instance.LoadLangModel(path)\n",
        "\t\n",
        "  def correct(self, text):\n",
        "    return self.instance.FixFragment(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoHKfJjeN5qD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spellchecker = Jamspell('ru_small.bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKYXr8ViQuH1",
        "colab_type": "text"
      },
      "source": [
        "## Preparing vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOUIKbApQt4C",
        "colab_type": "code",
        "outputId": "294ecb98-11da-4d01-cb72-aabb4fefdbd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!wget 'http://vectors.nlpl.eu/repository/11/185.zip'\n",
        "!unzip '185.zip'"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-25 18:46:32--  http://vectors.nlpl.eu/repository/11/185.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 639268530 (610M) [application/zip]\n",
            "Saving to: ‘185.zip’\n",
            "\n",
            "185.zip             100%[===================>] 609.65M  17.1MB/s    in 40s     \n",
            "\n",
            "2019-12-25 18:47:14 (15.1 MB/s) - ‘185.zip’ saved [639268530/639268530]\n",
            "\n",
            "Archive:  185.zip\n",
            "  inflating: README                  \n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW1RH5wQQwWC",
        "colab_type": "code",
        "outputId": "3152dcd7-f255-4984-cc3e-50ff644be201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "source": [
        "!wget https://www.dropbox.com/s/q1w4ftavn69383c/mean_vectors.pk?dl=0\n",
        "!mv mean_vectors.pk?dl=0 mean_vectors.pk"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-25 18:47:29--  https://www.dropbox.com/s/q1w4ftavn69383c/mean_vectors.pk?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.1, 2620:100:6016:1::a27d:101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/q1w4ftavn69383c/mean_vectors.pk [following]\n",
            "--2019-12-25 18:47:30--  https://www.dropbox.com/s/raw/q1w4ftavn69383c/mean_vectors.pk\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc941733d0853d8981a25d415ea6.dl.dropboxusercontent.com/cd/0/inline/Au5yZsi0yj1ID9TKO89Xtg_k2MNZzlcOHz1lwtwT_WYAaVI2niSxjQMj5Vl8iwGGu3SJwV5HJvGZckx-53GqK2wnTtxDb6sJXdOE8SHb712CsB7cN5KNO6qPEH8FdrJF8YQ/file# [following]\n",
            "--2019-12-25 18:47:30--  https://uc941733d0853d8981a25d415ea6.dl.dropboxusercontent.com/cd/0/inline/Au5yZsi0yj1ID9TKO89Xtg_k2MNZzlcOHz1lwtwT_WYAaVI2niSxjQMj5Vl8iwGGu3SJwV5HJvGZckx-53GqK2wnTtxDb6sJXdOE8SHb712CsB7cN5KNO6qPEH8FdrJF8YQ/file\n",
            "Resolving uc941733d0853d8981a25d415ea6.dl.dropboxusercontent.com (uc941733d0853d8981a25d415ea6.dl.dropboxusercontent.com)... 162.125.1.6, 2620:100:6016:6::a27d:106\n",
            "Connecting to uc941733d0853d8981a25d415ea6.dl.dropboxusercontent.com (uc941733d0853d8981a25d415ea6.dl.dropboxusercontent.com)|162.125.1.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/Au6B-1V-I0Fqi57HoB3MgoaElUHHbI6QnK--llEpevMCdXTdolX4Aw1aagXCeQHM1Tp92gvjckThujxxBtcZ-urRUt1_pSNxesyvOS4BzPhzWvh5ApdD022Qp7RhIG0TuhDbs78L8v9QRilAEa6Bc0uOEq1EL2KPZUhvQ9DjI9Wr_140Igowzr8n5hr_kaT2i3c3UCGnOSJDQNj8OIcprmjBT_L7nuOnkMKi1tAdP2sp5GshCOvIaqb_RpQ5a634HG6HlgaJQx4QSmJGKp5IMwl3aBNjjCOIJTtuhNMrKXrXLSqpHF50_j8PPJGFYTTgzHoK7LK89WwXRe8Yi70am3Wj4E12UVq_UU8IpcQzGuHMMQ/file [following]\n",
            "--2019-12-25 18:47:30--  https://uc941733d0853d8981a25d415ea6.dl.dropboxusercontent.com/cd/0/inline2/Au6B-1V-I0Fqi57HoB3MgoaElUHHbI6QnK--llEpevMCdXTdolX4Aw1aagXCeQHM1Tp92gvjckThujxxBtcZ-urRUt1_pSNxesyvOS4BzPhzWvh5ApdD022Qp7RhIG0TuhDbs78L8v9QRilAEa6Bc0uOEq1EL2KPZUhvQ9DjI9Wr_140Igowzr8n5hr_kaT2i3c3UCGnOSJDQNj8OIcprmjBT_L7nuOnkMKi1tAdP2sp5GshCOvIaqb_RpQ5a634HG6HlgaJQx4QSmJGKp5IMwl3aBNjjCOIJTtuhNMrKXrXLSqpHF50_j8PPJGFYTTgzHoK7LK89WwXRe8Yi70am3Wj4E12UVq_UU8IpcQzGuHMMQ/file\n",
            "Reusing existing connection to uc941733d0853d8981a25d415ea6.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5150 (5.0K) [application/x-tex-pk]\n",
            "Saving to: ‘mean_vectors.pk?dl=0’\n",
            "\n",
            "mean_vectors.pk?dl= 100%[===================>]   5.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-12-25 18:47:30 (769 MB/s) - ‘mean_vectors.pk?dl=0’ saved [5150/5150]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXtCHMWyQrSC",
        "colab_type": "code",
        "outputId": "b92d8913-c019-4c8d-a4d8-c8c8fb58520c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "w2v_model = KeyedVectors.load_word2vec_format('model.bin', binary=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MX77gHWY-ts",
        "colab_type": "text"
      },
      "source": [
        "## Downloading wordlists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9pEjTjzf43v",
        "colab_type": "code",
        "outputId": "11816cde-d67d-4b1b-c666-41c37f1e75fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "!wget https://www.dropbox.com/s/x6yiaveef5snf94/lexicon_by_parts.zip?dl=0\n",
        "!mv lexicon_by_parts.zip?dl=0 lexicon_by_parts.zip\n",
        "!unzip lexicon_by_parts.zip"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-25 18:47:38--  https://www.dropbox.com/s/x6yiaveef5snf94/lexicon_by_parts.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.1, 2620:100:6016:1::a27d:101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/x6yiaveef5snf94/lexicon_by_parts.zip [following]\n",
            "--2019-12-25 18:47:38--  https://www.dropbox.com/s/raw/x6yiaveef5snf94/lexicon_by_parts.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc9b2059b876f2e025fd6b3d18f5.dl.dropboxusercontent.com/cd/0/inline/Au4rjPgtBpUpgJrDK337oE8hlY9PfFak9vTA4hNsdSzTAZX7ip1jZNiwmR23IU2i9B8Lf6Z6YygVAuymaKRDAmde8wd36TZhuwDnHkXLXpCl9u06GSkV3qICTlOa05D89Wk/file# [following]\n",
            "--2019-12-25 18:47:38--  https://uc9b2059b876f2e025fd6b3d18f5.dl.dropboxusercontent.com/cd/0/inline/Au4rjPgtBpUpgJrDK337oE8hlY9PfFak9vTA4hNsdSzTAZX7ip1jZNiwmR23IU2i9B8Lf6Z6YygVAuymaKRDAmde8wd36TZhuwDnHkXLXpCl9u06GSkV3qICTlOa05D89Wk/file\n",
            "Resolving uc9b2059b876f2e025fd6b3d18f5.dl.dropboxusercontent.com (uc9b2059b876f2e025fd6b3d18f5.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:6016:6::a27d:106\n",
            "Connecting to uc9b2059b876f2e025fd6b3d18f5.dl.dropboxusercontent.com (uc9b2059b876f2e025fd6b3d18f5.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/Au6LBcd1tUl4RZtXnBiMLuhiHlTGrm7Q9cHU7pIDKvMJlK63FyLG67QHSeIbEJzDrBPBYEZauxURph9O28gZcSewSSCXQHVxy2ATHAIOpekmj9ztNr6W5Wv2DFtTJZ0IjNf-NezVbyyhahEAeycIoFRGgUDLfppypncVswlkhORsl1zd7HHy1j3NFqoH8MkrY8LN28KjHSHIKXVglqYcoqvvM2hpxoUE8rmRt5lyzYEdk7r9gbbSog6ap7HNeLbX3scJswzTUBStt0btMDAlSVr-Q2HZl5z2qiVkJAyBTpIvttTpOiIQFJxBiNXHRWToAOANvI1KENXQWZB3c27InTLZvFUh23rt8HF-SlPj2LPhZQ/file [following]\n",
            "--2019-12-25 18:47:39--  https://uc9b2059b876f2e025fd6b3d18f5.dl.dropboxusercontent.com/cd/0/inline2/Au6LBcd1tUl4RZtXnBiMLuhiHlTGrm7Q9cHU7pIDKvMJlK63FyLG67QHSeIbEJzDrBPBYEZauxURph9O28gZcSewSSCXQHVxy2ATHAIOpekmj9ztNr6W5Wv2DFtTJZ0IjNf-NezVbyyhahEAeycIoFRGgUDLfppypncVswlkhORsl1zd7HHy1j3NFqoH8MkrY8LN28KjHSHIKXVglqYcoqvvM2hpxoUE8rmRt5lyzYEdk7r9gbbSog6ap7HNeLbX3scJswzTUBStt0btMDAlSVr-Q2HZl5z2qiVkJAyBTpIvttTpOiIQFJxBiNXHRWToAOANvI1KENXQWZB3c27InTLZvFUh23rt8HF-SlPj2LPhZQ/file\n",
            "Reusing existing connection to uc9b2059b876f2e025fd6b3d18f5.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19060 (19K) [application/zip]\n",
            "Saving to: ‘lexicon_by_parts.zip?dl=0’\n",
            "\n",
            "lexicon_by_parts.zi 100%[===================>]  18.61K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-12-25 18:47:39 (241 MB/s) - ‘lexicon_by_parts.zip?dl=0’ saved [19060/19060]\n",
            "\n",
            "Archive:  lexicon_by_parts.zip\n",
            "   creating: lexicon_by_parts/\n",
            "  inflating: lexicon_by_parts/bigrams_food_negative.txt  \n",
            "  inflating: lexicon_by_parts/bigrams_food_positive.txt  \n",
            "  inflating: lexicon_by_parts/bigrams_service_negative.txt  \n",
            "  inflating: lexicon_by_parts/bigrams_service_positive.txt  \n",
            "  inflating: lexicon_by_parts/unigrams_food_negative.txt  \n",
            "  inflating: lexicon_by_parts/unigrams_food_positive.txt  \n",
            "  inflating: lexicon_by_parts/unigrams_service_negative.txt  \n",
            "  inflating: lexicon_by_parts/unigrams_service_positive.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58rv9saGZCu_",
        "colab_type": "text"
      },
      "source": [
        "## Building evaluator pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ICANHQiRbDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbfUCKb8e4A6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Evaluator:\n",
        "\n",
        "  def load_wordlist_uni(self, path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as inf:\n",
        "      wordset = set([w.split(\"\\t\")[1] for w in inf.read().split(\"\\n\") if w])\n",
        "    return wordset\n",
        "  \n",
        "  def load_wordlist_bi(self, path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as inf:\n",
        "      wordset = set([eval(w.split(\"\\t\")[1]) for w in inf.read().split(\"\\n\") if w])\n",
        "    return wordset\n",
        "\n",
        "  def __init__(self, ud_model, w2v_model, spellchecker, lists_path, vectors_path):\n",
        "    self._upch = 200\n",
        "    self.food_0_uni = self.load_wordlist_uni(os.path.join(lists_path, \"unigrams_food_negative.txt\"))\n",
        "    self.food_1_uni = self.load_wordlist_uni(os.path.join(lists_path, \"unigrams_food_positive.txt\"))\n",
        "    self.food_0_bi = self.load_wordlist_bi(os.path.join(lists_path, \"bigrams_food_negative.txt\"))\n",
        "    self.food_1_bi = self.load_wordlist_bi(os.path.join(lists_path, \"bigrams_food_positive.txt\"))\n",
        "    self.service_0_uni = self.load_wordlist_uni(os.path.join(lists_path, \"unigrams_service_negative.txt\"))\n",
        "    self.service_1_uni = self.load_wordlist_uni(os.path.join(lists_path, \"unigrams_service_positive.txt\"))\n",
        "    self.service_0_bi = self.load_wordlist_bi(os.path.join(lists_path, \"bigrams_service_negative.txt\"))\n",
        "    self.service_1_bi = self.load_wordlist_bi(os.path.join(lists_path, \"bigrams_service_positive.txt\"))\n",
        "    self.model = ud_model\n",
        "    self.spellchecker = spellchecker\n",
        "    self.w2v_model = w2v_model\n",
        "    with open(vectors_path, 'rb') as f:\n",
        "        mean_vectors_dict = pickle.load(f)\n",
        "\n",
        "    self.pos_food = mean_vectors_dict['pos_food']\n",
        "    self.neg_food = mean_vectors_dict['neg_food']\n",
        "    self.pos_service = mean_vectors_dict['pos_service']\n",
        "    self.neg_service = mean_vectors_dict['neg_service']\n",
        "\n",
        "  def printstr(self, coded_str):\n",
        "    sp = coded_str.split(\"\\t\")\n",
        "    sent_id = ord(sp[0]) - self._upch\n",
        "    word_id = ord(sp[1]) - self._upch\n",
        "    out = \"\\t\".join([str(sent_id), str(word_id), \"\\t\".join(sp[2:])])\n",
        "    return out\n",
        "\n",
        "  def vector_comparison(self, mean_vec_spec, mean_vec_comp, word_lemma):\n",
        "    try:\n",
        "      grammeme_vec = self.w2v_model[word_lemma]\n",
        "    except:\n",
        "      grammeme_vec = np.zeros((1,300))\n",
        "    grammeme_vec = grammeme_vec.reshape(1, -1)\n",
        "    \n",
        "    cos_sim_spec = cosine_similarity(mean_vec_spec.reshape(1, -1), grammeme_vec)[0][0]\n",
        "    cos_sim_comp = cosine_similarity(mean_vec_comp.reshape(1, -1), grammeme_vec)[0][0]\n",
        "    \n",
        "    if cos_sim_spec > 0.5 and (cos_sim_spec - cos_sim_comp) > 0.2:\n",
        "      result = True\n",
        "    else:\n",
        "      result = False\n",
        "    return result\n",
        "\n",
        "  def process_uni(self, word):\n",
        "    out = []\n",
        "    grammeme = word.lemma + \"_\" + word.pos\n",
        "    if grammeme in self.food_0_uni or self.vector_comparison(self.neg_food, self.pos_food, grammeme):\n",
        "      out.append(chr(self._upch + word.position) + \"\\tFood\\t0\")\n",
        "    if grammeme in self.food_1_uni or self.vector_comparison(self.pos_food, self.neg_food, grammeme):\n",
        "      out.append(chr(self._upch + word.position) + \"\\tFood\\t1\")\n",
        "    if grammeme in self.service_0_uni or self.vector_comparison(self.neg_service, self.pos_service, grammeme):\n",
        "      out.append(chr(self._upch + word.position) + \"\\tService\\t0\")\n",
        "    if grammeme in self.service_1_uni or self.vector_comparison(self.pos_service, self.neg_service, grammeme):\n",
        "      out.append(chr(self._upch + word.position) + \"\\tService\\t1\")\n",
        "    \n",
        "    return out\n",
        "\n",
        "  def process_bi(self, bigram):\n",
        "    out = []\n",
        "    gramm_pair = (bigram[0].lemma + \"_\" + bigram[0].pos, bigram[1].lemma + \"_\" + bigram[1].pos)\n",
        "    if gramm_pair in self.food_0_bi:\n",
        "      out.append(chr(self._upch + bigram[0].position) + \"\\tFood\\t0\")\n",
        "      out.append(chr(self._upch + bigram[1].position) + \"\\tFood\\t0\")\n",
        "    if gramm_pair in self.food_1_bi:\n",
        "      out.append(chr(self._upch + bigram[0].position) + \"\\tFood\\t1\")\n",
        "      out.append(chr(self._upch + bigram[1].position) + \"\\tFood\\t1\")\n",
        "    if gramm_pair in self.service_0_bi:\n",
        "      out.append(chr(self._upch + bigram[0].position) + \"\\tService\\t0\")\n",
        "      out.append(chr(self._upch + bigram[1].position) + \"\\tService\\t0\")\n",
        "    if gramm_pair in self.service_1_bi:\n",
        "      out.append(chr(self._upch + bigram[0].position) + \"\\tService\\t1\")\n",
        "      out.append(chr(self._upch + bigram[1].position) + \"\\tService\\t1\")\n",
        "    return out\n",
        "\n",
        "  def process_text(self, text):\n",
        "    text = self.spellchecker.correct(text)\n",
        "    text_parse = self.model.parse_text(text)\n",
        "    out = []\n",
        "    for sent_id in text_parse:\n",
        "      SID = chr(self._upch + sent_id)\n",
        "      sent = text_parse[sent_id]\n",
        "      out += [SID + \"\\t\" + res for res in self.process_uni(sent[0])]\n",
        "      bigrams = [(sent[i], sent[i+1]) for i in range(len(sent) - 1)]\n",
        "      for bigram in bigrams:\n",
        "          out += [SID + \"\\t\" + res for res in self.process_bi(bigram)]\n",
        "          out += [SID + \"\\t\" + res for res in self.process_uni(bigram[1])]\n",
        "    out = sorted(list(set(out)))\n",
        "    outstr = \"\\n\".join(self.printstr(s) for s in out)\n",
        "    return outstr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq8y4vFJQ8C8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluator = Evaluator(ud_model, w2v_model, spellchecker, './lexicon_by_parts', 'mean_vectors.pk')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL-C5-_kZIv5",
        "colab_type": "text"
      },
      "source": [
        "## Predicting!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbAYiVe-Rzkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_text = \"\"\"— Тут был хороший бо-орщь, с капусткой, но не красный. Так... Сасисачки. Ну, ещё есть какой-то непонятный салаД, куда крошаД морковку, капусту и яблоки с ананасами (смеётся). Вообще он меня бесит. Вот... Эщо чоо... Вкусный чай, он так уталяет жажду (машет руками), я чувствую себя человеком! Вот. Всё.\n",
        "— (голос за кадром) Как фамилия ваша?\n",
        "— Я Никита Литвинков. (реверанс)\n",
        "— (голос за кадром) Спасибо...\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVrAjR4VRlyf",
        "colab_type": "code",
        "outputId": "370e9b5e-2096-4d6b-96f2-7bf1a8945787",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(evaluator.process_text(sample_text))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\t3\tService\t1\n",
            "1\t4\tService\t1\n",
            "8\t1\tFood\t1\n",
            "11\t15\tService\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40M8xuIBwJrV",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG1v-eeWwdff",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "outputId": "90032372-7b5d-4620-a7ad-da13bb649bb4"
      },
      "source": [
        "!apt-get install subversion"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libapr1 libaprutil1 libserf-1-1 libsvn1\n",
            "Suggested packages:\n",
            "  db5.3-util libapache2-mod-svn subversion-tools\n",
            "The following NEW packages will be installed:\n",
            "  libapr1 libaprutil1 libserf-1-1 libsvn1 subversion\n",
            "0 upgraded, 5 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 2,237 kB of archives.\n",
            "After this operation, 9,910 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libapr1 amd64 1.6.3-2 [90.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libaprutil1 amd64 1.6.1-2 [84.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libserf-1-1 amd64 1.3.9-6 [44.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsvn1 amd64 1.9.7-4ubuntu1 [1,183 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 subversion amd64 1.9.7-4ubuntu1 [834 kB]\n",
            "Fetched 2,237 kB in 2s (1,408 kB/s)\n",
            "Selecting previously unselected package libapr1:amd64.\n",
            "(Reading database ... 135795 files and directories currently installed.)\n",
            "Preparing to unpack .../libapr1_1.6.3-2_amd64.deb ...\n",
            "Unpacking libapr1:amd64 (1.6.3-2) ...\n",
            "Selecting previously unselected package libaprutil1:amd64.\n",
            "Preparing to unpack .../libaprutil1_1.6.1-2_amd64.deb ...\n",
            "Unpacking libaprutil1:amd64 (1.6.1-2) ...\n",
            "Selecting previously unselected package libserf-1-1:amd64.\n",
            "Preparing to unpack .../libserf-1-1_1.3.9-6_amd64.deb ...\n",
            "Unpacking libserf-1-1:amd64 (1.3.9-6) ...\n",
            "Selecting previously unselected package libsvn1:amd64.\n",
            "Preparing to unpack .../libsvn1_1.9.7-4ubuntu1_amd64.deb ...\n",
            "Unpacking libsvn1:amd64 (1.9.7-4ubuntu1) ...\n",
            "Selecting previously unselected package subversion.\n",
            "Preparing to unpack .../subversion_1.9.7-4ubuntu1_amd64.deb ...\n",
            "Unpacking subversion (1.9.7-4ubuntu1) ...\n",
            "Setting up libapr1:amd64 (1.6.3-2) ...\n",
            "Setting up libaprutil1:amd64 (1.6.1-2) ...\n",
            "Setting up libserf-1-1:amd64 (1.3.9-6) ...\n",
            "Setting up libsvn1:amd64 (1.9.7-4ubuntu1) ...\n",
            "Setting up subversion (1.9.7-4ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsY58MpzwJWU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "outputId": "04d2ab3e-52f1-46b2-a822-6a9f58e5c64d"
      },
      "source": [
        "!svn checkout https://github.com/sjut/HSE-Compling/trunk/hw/texts"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A    texts/01001.txt\n",
            "A    texts/01002.txt\n",
            "A    texts/01003.txt\n",
            "A    texts/01004.txt\n",
            "A    texts/01005.txt\n",
            "A    texts/01006.txt\n",
            "A    texts/01007.txt\n",
            "A    texts/01008.txt\n",
            "A    texts/01009.txt\n",
            "A    texts/01010.txt\n",
            "A    texts/1032.txt\n",
            "A    texts/12341.txt\n",
            "A    texts/12943.txt\n",
            "A    texts/13823.txt\n",
            "A    texts/1515.txt\n",
            "A    texts/15159.txt\n",
            "A    texts/17398.txt\n",
            "A    texts/19383.txt\n",
            "A    texts/20086.txt\n",
            "A    texts/22975.txt\n",
            "A    texts/24578.txt\n",
            "A    texts/24916.txt\n",
            "A    texts/26284.txt\n",
            "A    texts/26330.txt\n",
            "A    texts/27629.txt\n",
            "A    texts/28083.txt\n",
            "A    texts/30808.txt\n",
            "A    texts/32840.txt\n",
            "A    texts/32856.txt\n",
            "A    texts/33520.txt\n",
            "A    texts/33591.txt\n",
            "A    texts/33693.txt\n",
            "A    texts/33838.txt\n",
            "A    texts/3385.txt\n",
            "A    texts/33976.txt\n",
            "A    texts/34282.txt\n",
            "A    texts/35486.txt\n",
            "A    texts/36926.txt\n",
            "A    texts/37036.txt\n",
            "A    texts/37516.txt\n",
            "A    texts/37819.txt\n",
            "A    texts/4063.txt\n",
            "A    texts/5037.txt\n",
            "A    texts/5648.txt\n",
            "A    texts/6376.txt\n",
            "A    texts/6668.txt\n",
            "A    texts/7079.txt\n",
            "A    texts/7821.txt\n",
            "A    texts/8759.txt\n",
            "A    texts/9036.txt\n",
            "Checked out revision 82.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ng93__YwyHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir scored"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6029tTOxQyo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "outputId": "fb3e2af9-f27a-49cf-bd39-ab7c4da55ba6"
      },
      "source": [
        "%%time\n",
        "\n",
        "texts = [fname for fname in os.listdir(\"texts\") if fname.endswith(\".txt\")]\n",
        "for fname in texts:\n",
        "  with open(os.path.join(\"texts\", fname), \"r\", encoding=\"utf-8\") as infile:\n",
        "    text = infile.read()\n",
        "  parsed = evaluator.process_text(text)\n",
        "  print(\"Processed\", fname)\n",
        "  with open(os.path.join(\"scored\", fname+\".scored\"), \"w\", encoding=\"utf-8\") as outfile:\n",
        "    outfile.write(parsed)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed 37819.txt\n",
            "Processed 37036.txt\n",
            "Processed 19383.txt\n",
            "Processed 01001.txt\n",
            "Processed 32840.txt\n",
            "Processed 6668.txt\n",
            "Processed 24578.txt\n",
            "Processed 33693.txt\n",
            "Processed 01003.txt\n",
            "Processed 12943.txt\n",
            "Processed 01005.txt\n",
            "Processed 26284.txt\n",
            "Processed 01008.txt\n",
            "Processed 30808.txt\n",
            "Processed 4063.txt\n",
            "Processed 7821.txt\n",
            "Processed 1515.txt\n",
            "Processed 01010.txt\n",
            "Processed 24916.txt\n",
            "Processed 33591.txt\n",
            "Processed 1032.txt\n",
            "Processed 13823.txt\n",
            "Processed 01002.txt\n",
            "Processed 34282.txt\n",
            "Processed 20086.txt\n",
            "Processed 01004.txt\n",
            "Processed 8759.txt\n",
            "Processed 3385.txt\n",
            "Processed 6376.txt\n",
            "Processed 27629.txt\n",
            "Processed 37516.txt\n",
            "Processed 33976.txt\n",
            "Processed 9036.txt\n",
            "Processed 32856.txt\n",
            "Processed 01007.txt\n",
            "Processed 5648.txt\n",
            "Processed 5037.txt\n",
            "Processed 33838.txt\n",
            "Processed 7079.txt\n",
            "Processed 28083.txt\n",
            "Processed 01006.txt\n",
            "Processed 22975.txt\n",
            "Processed 17398.txt\n",
            "Processed 26330.txt\n",
            "Processed 35486.txt\n",
            "Processed 33520.txt\n",
            "Processed 36926.txt\n",
            "Processed 15159.txt\n",
            "Processed 01009.txt\n",
            "Processed 12341.txt\n",
            "CPU times: user 35 s, sys: 426 ms, total: 35.4 s\n",
            "Wall time: 34.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBTGoDHpyzCn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "11e676e2-0e33-43a2-f208-2c089625b582"
      },
      "source": [
        "!cat scored/01009.txt.scored"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\t3\tService\t1\n",
            "5\t4\tService\t1\n",
            "10\t12\tService\t1\n",
            "16\t3\tFood\t1\n",
            "16\t4\tFood\t1\n",
            "18\t2\tFood\t0\n",
            "18\t3\tFood\t0\n",
            "18\t4\tFood\t0\n",
            "20\t2\tService\t1\n",
            "20\t3\tFood\t1\n",
            "20\t3\tService\t1"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aAARB29ykqR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "7db22e84-d53b-4488-9ac4-ee1670d88f55"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmJWApnfyoRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r scored /content/gdrive/My\\ Drive/scored/group_2"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}